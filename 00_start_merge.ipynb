{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's import the nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from multiset import *\n",
    "from string import punctuation\n",
    "from datasets import load_dataset_builder\n",
    "from datasets import load_dataset\n",
    "\n",
    "from functools import reduce\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "The following cells will execute the code given in the hugging face tutorial to explore the dataset\n",
    "and answer the different questions around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_builder = load_dataset_builder(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_builder.info.description)\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dataset[\"validation\"][\"label\"]).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier \n",
    "The next cells will focus on the Naive Bayes classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will preprocces the dataset, lowering every character and removing unwanted ponctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pre-treat the input by removing the puntuation and lowering it\n",
    "\n",
    "# We also transfer it into pandas Dataframe format to ease handling \n",
    "all_data = {}\n",
    "\n",
    "def clean_data(text_data):\n",
    "    text_data = text_data.str.lower()\n",
    "    text_data = text_data.str.replace(\"[\" + punctuation + \"]( |$)\", \"\", regex=True)\n",
    "    return text_data\n",
    "\n",
    "for type in dataset.keys():\n",
    "        \n",
    "    text_data = pd.Series(dataset[type][\"text\"])\n",
    "\n",
    "    text_data = clean_data(text_data=text_data)\n",
    "    all_data[type] = pd.concat([text_data, pd.Series(dataset[type][\"label\"])], axis=1)\n",
    "    all_data[type].columns=[\"text\", \"label\"]\n",
    "    \n",
    "    \n",
    "data = all_data[\"train\"]\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize words with a multiset data structure (bag of word method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vocabulary(text_data: pd.DataFrame):\n",
    "    td = text_data.copy()\n",
    "    td[\"text\"] = td[\"text\"].apply(lambda t: Multiset(t.split(\" \")))\n",
    "    \n",
    "    return td.groupby(by=\"label\")[\"text\"].sum()\n",
    "\n",
    "V_c = find_vocabulary(data)\n",
    "V = V_c.sum()\n",
    "\n",
    "C = [0, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the Train Naive Bayes function dicribed in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data: pd.DataFrame, classes: list):\n",
    "    V_c = find_vocabulary(data) # called bigdoc in pseudo code\n",
    "    V = V_c.sum()\n",
    "    \n",
    "    # creates empty df where we will store our word occurence conditional probabilities \n",
    "    logLikelihood = pd.DataFrame(index=pd.Index(set(V)), columns=classes) \n",
    "\n",
    "    logprior = {}\n",
    "    n_data = len(data)\n",
    "    for c in classes:\n",
    "        n_c = data[\"label\"].value_counts()[c]\n",
    "        logprior[c] = math.log(n_c / n_data)\n",
    "        \n",
    "        cardinal_V = (len(V) + len(V.items()))\n",
    "        \n",
    "        for w in V:\n",
    "            count = V_c[c][w]\n",
    "            logLikelihood.loc[w, c] = math.log((count + 1) / cardinal_V)\n",
    "            \n",
    "    return logprior, logLikelihood\n",
    "            \n",
    "\n",
    "\n",
    "logprior, logLikelihood = train(data, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_args = (logprior, logLikelihood, C, V)\n",
    "\n",
    "#Implements Test Naive Bayes function\n",
    "def predict(doc, logprior, logLikelihood, C, V):\n",
    "    sumed = {}\n",
    "    for c in C:\n",
    "        sumed[c] = logprior[c]\n",
    "        for w in doc.split(\" \"):\n",
    "            if w in V:\n",
    "                sumed[c] += logLikelihood.loc[w, c]\n",
    "\n",
    "    return max(sumed, key=sumed.get)\n",
    "\n",
    "# Applies prediction to a whole df\n",
    "def predict_dataset(data, logprior, logLikelihood, C, V):\n",
    "    data[\"results\"] = (\n",
    "        data[\"text\"].apply(lambda t: predict(t, logprior, logLikelihood, C, V))\n",
    "        == data[\"label\"]\n",
    "    )\n",
    "\n",
    "# Print the percent of right answer\n",
    "def evaluate(data, name, logprior, logLikelihood, C, V):\n",
    "    print(f\"results on {name} dataset : \", data[\"results\"].value_counts()[True] / len(data))\n",
    "\n",
    "# Predicts and evaluate on all our dataset\n",
    "for type in dataset.keys():\n",
    "    predict_dataset(all_data[type], *predict_args)   \n",
    "    evaluate(all_data[type], type, *predict_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cells build the Naive Bayes Classifier using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "estimator = [('Cv', CountVectorizer()), ('Bayes', MultinomialNB())]\n",
    "pipe = Pipeline(estimator)\n",
    "pipe.fit(data.text, data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in dataset.keys():  \n",
    "    print(f\"{type} : {pipe.score(all_data[type]['text'],all_data[type]['label'])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most likely, the scikit-learn implementation will give better results. Looking at the documentation, explain why it could be the case.**\n",
    "\n",
    "Sklearn seems better because it uses a Laplace smoothing parameter.\n",
    "\n",
    "To test this hypotheis I run the same model but with no smoothing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = [('Cv', CountVectorizer()), ('Bayes', MultinomialNB(alpha=0, force_alpha=True))]\n",
    "pipeNoalpha = Pipeline(estimator)\n",
    "pipeNoalpha.fit(data.text, data.label)\n",
    "\n",
    "for type in dataset.keys():  \n",
    "    print(f\"{type} : {pipe.score(all_data[type]['text'],all_data[type]['label'])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy droped on test and was higher on train, the model seems to overfit.\n",
    "\n",
    "The smoothing parameter is definitely important to get a better accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is accuracy a sufficient measure of evaluation here?**\n",
    "\n",
    "It is a sufficient measure because the classes or equally split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.predict(all_data[\"train\"]['text'])\n",
    "\n",
    "mask = prediction != all_data['train']['label']\n",
    "# convert the mask to a pandas series\n",
    "series = pd.Series(mask)\n",
    "\n",
    "series.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = all_data['train'].loc[series]\n",
    "\n",
    "def printWrong(row : pd.Series):\n",
    "    print(f\"We were wrong on  : {row['text']}\\nReal label : {'positive' if row['label'] else 'negative'}\")\n",
    "\n",
    "wrong.head().apply(printWrong, axis=1)\n",
    "\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. [Bonus] What are the top 10 most important words (features) for each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [0, 1]:\n",
    "    print(f\"Most impactfull words in class {c} :\")\n",
    "    display(logLikelihood.sort_values(by=c, ascending=False)[c].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the stopword list\n",
    "import ssl\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.add('')\n",
    "print(stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for c in [0, 1]:\n",
    "    print(f\"Most impactfull words in class {c} (exepting stopwords):\")\n",
    "    most_impact = logLikelihood.sort_values(by=c, ascending=False)[c]\n",
    "    display(most_impact.loc[~most_impact.index.isin(stops)].head(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at scikit's log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = pd.Series(pipe[\"Cv\"].get_feature_names_out(), name=\"name\")\n",
    "scikit_log_prob = pd.DataFrame(pipe[\"Bayes\"].feature_log_prob_).transpose()\n",
    "scikit_log_prob.columns = pipe[\"Bayes\"].classes_\n",
    "scikit_log_prob.index = names\n",
    "\n",
    "for c in pipe[\"Bayes\"].classes_:\n",
    "    print(f\"Most impactfull words in class {c} in scikit learn :\")\n",
    "    most_impact = scikit_log_prob.sort_values(by=c, ascending=False)[c]\n",
    "    display(most_impact.head(10))\n",
    "\n",
    "    print(\"And without stop words :\")\n",
    "\n",
    "    display(most_impact.loc[~most_impact.index.isin(stops)].head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. [Bonus] Play with scikit-learn's version parameters. For example, see if you can consider unigram and bigram instead of only unigrams.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
